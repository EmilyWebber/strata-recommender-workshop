{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall keras -y\n",
    "# !pip install keras==2.1.2\n",
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "import keras\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(f_name):\n",
    "    \n",
    "    df = pd.read_csv(f_name, sep='\\t', nrows = 10000)\n",
    "        \n",
    "    df['docs_to_embed'] = df['product_title'] + ', ' + df['review_headline'] + ', ' + df['review_body']\n",
    "    \n",
    "    df.drop(['product_title', 'review_headline', 'review_body'], axis=1, inplace=True)\n",
    "    \n",
    "    # remove nans\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "#     df['customer_id'] = str(df['customer_id'])\n",
    " \n",
    "    return df\n",
    "\n",
    "df = read_data('micro_data.tsv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a model\n",
    "\n",
    "### Features to embed:\n",
    "1. Product title\n",
    "2. Review headline\n",
    "3. Review body\n",
    "4. Customer ID\n",
    "5. Product ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    labels = [1 if int(x) >= 4 else 0 for x in df['star_rating'] ]\n",
    "    return labels\n",
    "\n",
    "labels = get_label(df)\n",
    "\n",
    "\n",
    "def get_encoded_ids(df, id_name):\n",
    "\n",
    "    assert id_name in ['product_id', 'docs_to_embed']\n",
    "    \n",
    "    vocab_size = get_vocab_size(df, col_name = id_name)\n",
    "\n",
    "    docs = df[id_name].values.tolist()\n",
    "        \n",
    "    encoded_ids = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "    return np.array(encoded_ids)\n",
    "\n",
    "def get_vocab_size(df, col_name):\n",
    "    vocab_size = len(set((' ').join(df[col_name]).split()))\n",
    "    return vocab_size\n",
    "\n",
    "def get_max_length(df, col_name):\n",
    "    max_length = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        doc = row[col_name]\n",
    "        l = len(doc.split())\n",
    "        if l > max_length:\n",
    "            max_length = l\n",
    "            \n",
    "    return max_length\n",
    "\n",
    "def get_padded_documents(df):\n",
    "    encoded_docs = get_encoded_ids(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, 'docs_to_embed')\n",
    "    \n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional inputs\n",
    "On top of looking at the textual data, we have a few additional columns we need to consider. Each of these we'll consider a running variable\n",
    "1. Product category\n",
    "2. Votes\n",
    "\n",
    "In the tiny sample dataset, all the records are on the same date and are in the same product category, so there's no reason to include either date or product category in the first model. We'll specify all of these as  inputs, then pass them in as a list to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_specs(df):\n",
    "    vocab_size = get_vocab_size(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, col_name = 'docs_to_embed')\n",
    "\n",
    "    n_users = len(set(df['customer_id'].values.tolist()))\n",
    "    n_products = len(set(df['product_id'].values.tolist()))\n",
    "\n",
    "    return vocab_size, max_length, n_users, n_products\n",
    "\n",
    "def get_scaled(df, col_name):\n",
    "    x = [int(x) for x in df[col_name]]\n",
    "    \n",
    "    x = np.reshape(x, (-1, 1))\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "\n",
    "    scaler_x.fit(x)\n",
    "    \n",
    "    xscale = scaler_x.transform(x)\n",
    "\n",
    "    return xscale\n",
    "\n",
    "def get_model_input_data(df):\n",
    "    \n",
    "    padded_docs = get_padded_documents(df)\n",
    "    \n",
    "    encoded_product_ids = get_encoded_ids(df, 'product_id')\n",
    "    \n",
    "    # failed here \n",
    "#     encoded_customer_idx = get_encoded_ids(df, 'customer_id')\n",
    "\n",
    "    votes = get_scaled(df, 'total_votes')\n",
    "    \n",
    "#     return [encoded_customer_idx, encoded_product_ids, padded_docs, votes]\n",
    "\n",
    "    return [padded_docs, votes, encoded_product_ids, df['customer_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(df):\n",
    "\n",
    "    vocab_size, max_length, n_users, n_products = get_model_input_specs(df)\n",
    "\n",
    "    ##########\n",
    "    # INPUTS #\n",
    "    ##########\n",
    "\n",
    "    doc_input = Input(shape=[max_length,], dtype='int32', name=\"Document-Input\")\n",
    "    doc_embedding = Embedding(vocab_size, output_dim = 512, name=\"Document-Embedding\", input_length=max_length)(doc_input)\n",
    "    lstm_out = LSTM(32)(doc_embedding)\n",
    "    # auxiliary output \n",
    "    aux_out = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "\n",
    "    votes_input = Input(shape=[1,], name=\"Votes-Input\") \n",
    "\n",
    "    product_input = Input(shape=[1, ], name=\"Product-Input\")\n",
    "    product_embedding = Embedding(n_products+1, 24, name=\"Product-Embedding\")(product_input)\n",
    "    # is it better to include this layer, or not? \n",
    "    product_vec = Flatten(name=\"Flatten-Products\")(product_embedding)\n",
    "\n",
    "    user_input = Input(shape=[1, ], name=\"User-Input\")\n",
    "#     user_embedding = Embedding(n_users+1, 24, name=\"User-Embedding\")(user_input)\n",
    "#     user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "    ##########\n",
    "    # CONCAT #\n",
    "    ########## \n",
    "    concat = keras.layers.concatenate([lstm_out, votes_input, product_vec, user_input], \n",
    "                                      name = 'main_concat')\n",
    "\n",
    "    x1 = Dense(64, activation='relu', name='1st_post_dense')(concat)\n",
    "\n",
    "    x2 = keras.layers.Dropout(.2, name='Dropout')(x1)\n",
    "\n",
    "    x3 = Dense(32, activation='relu', name='3st_post_dense')(x2)\n",
    "\n",
    "    ###############\n",
    "    # PREDICTIONS #\n",
    "    ###############\n",
    "\n",
    "    predictions = keras.layers.Dense(1, activation='sigmoid')(x3)\n",
    "\n",
    "    #########\n",
    "    # MODEL #\n",
    "    #########\n",
    "\n",
    "    # your model is a list of embedded inputs, then the dot product, then the scaled running variables \n",
    "#     model = Model(inputs = [doc_input, votes_input, product_input, user_input], output = predictions)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs = [doc_input, votes_input, product_input, user_input], output = predictions)\n",
    "\n",
    "\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 19:51:40.300965 139845503002432 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0918 19:51:40.318198 139845503002432 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0918 19:51:40.327623 139845503002432 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0918 19:51:40.651489 139845503002432 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0918 19:51:40.656479 139845503002432 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0918 19:51:40.682864 139845503002432 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0918 19:51:40.696653 139845503002432 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0918 19:51:40.700061 139845503002432 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = get_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = get_model_input_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 3096s 387ms/step - loss: 5.6218 - val_loss: 4.2486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_input, labels, epochs=1, verbose=1, validation_split=0.2)   \n",
    "model.save('/home/ec2-user/SageMaker/model-versions/full-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a visual representation of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a visual representation of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"full-model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"full-model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
