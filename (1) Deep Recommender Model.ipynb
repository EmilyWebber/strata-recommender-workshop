{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Recommender Model\n",
    "In this notebook we'll:\n",
    "- Read in the data from disk\n",
    "- Define the preprocessing functions\n",
    "- Define the model\n",
    "- Apply the features to the model, ie fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling Keras-2.1.2:\n",
      "  Successfully uninstalled Keras-2.1.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras==2.1.2\n",
      "  Using cached https://files.pythonhosted.org/packages/68/89/58ee5f56a9c26957d97217db41780ebedca3154392cb903c3f8a08a52208/Keras-2.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.16.4)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras -y\n",
    "!pip install keras==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "import sagemaker\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense, LSTM, Activation\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('''gunzip micro_data.tsv.gz''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(f_name):\n",
    "    \n",
    "    df = pd.read_csv(f_name, sep='\\t', nrows = 2000)\n",
    "        \n",
    "    df['docs_to_embed'] = df['product_title'] + ', ' + df['review_headline'] + ', ' + df['review_body']\n",
    "    \n",
    "    df.drop(['product_title', 'review_headline', 'review_body'], axis=1, inplace=True)\n",
    "    \n",
    "    # remove nans\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "#     df['customer_id'] = str(df['customer_id'])\n",
    " \n",
    "    return df\n",
    "\n",
    "df = read_data('micro_data.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a model\n",
    "\n",
    "### Features to embed:\n",
    "1. Product title\n",
    "2. Review headline\n",
    "3. Review body\n",
    "4. Customer ID\n",
    "5. Product ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    labels = [1 if int(x) >= 4 else 0 for x in df['star_rating'] ]\n",
    "    return labels\n",
    "\n",
    "labels = get_label(df)\n",
    "\n",
    "def get_encoded_ids(df, id_name):\n",
    "\n",
    "    assert id_name in ['product_id', 'docs_to_embed']\n",
    "    \n",
    "    vocab_size = get_vocab_size(df, col_name = id_name)\n",
    "    \n",
    "    print (id_name, vocab_size)\n",
    "\n",
    "    docs = df[id_name].values.tolist()\n",
    "        \n",
    "    encoded_ids = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "    return np.array(encoded_ids)\n",
    "\n",
    "def get_vocab_size(df, col_name):\n",
    "    vocab_size = len(set((' ').join(df[col_name]).split()))\n",
    "    return vocab_size\n",
    "\n",
    "def get_max_length(df, col_name):\n",
    "    max_length = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        doc = row[col_name]\n",
    "        l = len(doc.split())\n",
    "        if l > max_length:\n",
    "            max_length = l\n",
    "            \n",
    "    return max_length\n",
    "\n",
    "def get_padded_documents(df):\n",
    "    encoded_docs = get_encoded_ids(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, 'docs_to_embed')\n",
    "    \n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional inputs\n",
    "On top of looking at the textual data, we have a few additional columns we need to consider. Each of these we'll consider a running variable\n",
    "1. Product category\n",
    "2. Votes\n",
    "\n",
    "In the tiny sample dataset, all the records are on the same date and are in the same product category, so there's no reason to include either date or product category in the first model. We'll specify all of these as  inputs, then pass them in as a list to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_specs(df):\n",
    "    vocab_size = get_vocab_size(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, col_name = 'docs_to_embed')\n",
    "\n",
    "    n_users = len(set(df['customer_id'].values.tolist()))\n",
    "    n_products = len(set(df['product_id'].values.tolist()))\n",
    "\n",
    "    return vocab_size, max_length, n_users, n_products\n",
    "\n",
    "def get_scaled(df, col_name):\n",
    "    x = [int(x) for x in df[col_name]]\n",
    "    \n",
    "    x = np.reshape(x, (-1, 1))\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "\n",
    "    scaler_x.fit(x)\n",
    "    \n",
    "    xscale = scaler_x.transform(x)\n",
    "\n",
    "    return xscale\n",
    "\n",
    "def get_model_input_data(df):\n",
    "    \n",
    "    padded_docs = get_padded_documents(df)\n",
    "    \n",
    "    encoded_product_ids = get_encoded_ids(df, 'product_id')\n",
    "\n",
    "    votes = get_scaled(df, 'total_votes')\n",
    "    \n",
    "    return [padded_docs, votes, encoded_product_ids, df['customer_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(df):\n",
    "\n",
    "    vocab_size, max_length, n_users, n_products = get_model_input_specs(df)\n",
    "\n",
    "    ##########\n",
    "    # INPUTS #\n",
    "    ##########\n",
    "\n",
    "    doc_input = Input(shape=[max_length,], dtype='int32', name=\"Document-Input\")\n",
    "    doc_embedding = Embedding(vocab_size, output_dim = 512, name=\"Document-Embedding\", input_length=max_length)(doc_input)\n",
    "    lstm_out = LSTM(32)(doc_embedding)\n",
    "    # auxiliary output \n",
    "    aux_out = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "\n",
    "    votes_input = Input(shape=[1,], name=\"Votes-Input\") \n",
    "\n",
    "    product_input = Input(shape=[1, ], name=\"Product-Input\")\n",
    "    product_embedding = Embedding(n_products+1, 24, name=\"Product-Embedding\")(product_input)\n",
    "    \n",
    "    product_vec = Flatten(name=\"Flatten-Products\")(product_embedding)\n",
    "\n",
    "    user_input = Input(shape=[1, ], name=\"User-Input\")\n",
    "\n",
    "    ##########\n",
    "    # CONCAT #\n",
    "    ########## \n",
    "    concat = keras.layers.concatenate([lstm_out, votes_input, product_vec, user_input], \n",
    "                                      name = 'main_concat')\n",
    "\n",
    "    x1 = Dense(64, activation='relu', name='1st_post_dense')(concat)\n",
    "\n",
    "    x2 = keras.layers.Dropout(.2, name='Dropout')(x1)\n",
    "\n",
    "    x3 = Dense(32, activation='relu', name='3st_post_dense')(x2)\n",
    "\n",
    "    ###############\n",
    "    # PREDICTIONS #\n",
    "    ###############\n",
    "\n",
    "    predictions = keras.layers.Dense(1, activation='sigmoid')(x3)\n",
    "\n",
    "    #########\n",
    "    # MODEL #\n",
    "    #########\n",
    "\n",
    "    model = Model(inputs = [doc_input, votes_input, product_input, user_input], output = predictions)\n",
    "\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you defined the model, run this next cell for a few minutes. The data is too large to train in a reasonable time period for a workshop, so you're actually going to load a pre-trained model. But run this step so you can see that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs_to_embed 12629\n",
      "product_id 1358\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 414s 259ms/step - loss: 5.4634 - val_loss: 3.8660\n"
     ]
    }
   ],
   "source": [
    "model = get_model(df)\n",
    "\n",
    "data_input = get_model_input_data(df)\n",
    "\n",
    "history = model.fit(data_input, labels, epochs=1, verbose=1, validation_split=0.2)  \n",
    "\n",
    "version = 8\n",
    "\n",
    "model.save('full-model-v{}.h5'.format(version))\n",
    "\n",
    "# write to json \n",
    "json_string = model.to_json()\n",
    "with open(\"full-model-v{}.json\".format(version), \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('full-model_weights-v{}.h5'.format(version))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
