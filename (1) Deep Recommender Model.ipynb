{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Recommender Model\n",
    "In this notebook we'll:\n",
    "- Read in the data from disk\n",
    "- Define the preprocessing functions\n",
    "- Define the model\n",
    "- Apply the features to the model, ie fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling Keras-2.2.4:\n",
      "  Successfully uninstalled Keras-2.2.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras==2.1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/89/58ee5f56a9c26957d97217db41780ebedca3154392cb903c3f8a08a52208/Keras-2.1.2-py2.py3-none-any.whl (304kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 31.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.16.4)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (3.12)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.11.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras -y\n",
    "!pip install keras==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "import sagemaker\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense, LSTM, Activation\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('''gunzip micro_data.tsv.gz''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(f_name):\n",
    "    \n",
    "    df = pd.read_csv(f_name, sep='\\t', nrows = 2000)\n",
    "        \n",
    "    df['docs_to_embed'] = df['product_title'] + ', ' + df['review_headline'] + ', ' + df['review_body']\n",
    "    \n",
    "    df.drop(['product_title', 'review_headline', 'review_body'], axis=1, inplace=True)\n",
    "    \n",
    "    # remove nans\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "#     df['customer_id'] = str(df['customer_id'])\n",
    " \n",
    "    return df\n",
    "\n",
    "df = read_data('micro_data.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a model\n",
    "\n",
    "### Features to embed:\n",
    "1. Product title\n",
    "2. Review headline\n",
    "3. Review body\n",
    "4. Customer ID\n",
    "5. Product ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    labels = [1 if int(x) >= 4 else 0 for x in df['star_rating'] ]\n",
    "    return labels\n",
    "\n",
    "labels = get_label(df)\n",
    "\n",
    "def get_encoded_ids(df, id_name):\n",
    "\n",
    "    assert id_name in ['product_id', 'docs_to_embed']\n",
    "    \n",
    "    vocab_size = get_vocab_size(df, col_name = id_name)\n",
    "    \n",
    "    print (id_name, vocab_size)\n",
    "\n",
    "    docs = df[id_name].values.tolist()\n",
    "        \n",
    "    encoded_ids = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "    return np.array(encoded_ids)\n",
    "\n",
    "def get_vocab_size(df, col_name):\n",
    "    vocab_size = len(set((' ').join(df[col_name]).split()))\n",
    "    return vocab_size\n",
    "\n",
    "def get_max_length(df, col_name):\n",
    "    max_length = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        doc = row[col_name]\n",
    "        l = len(doc.split())\n",
    "        if l > max_length:\n",
    "            max_length = l\n",
    "            \n",
    "    return max_length\n",
    "\n",
    "def get_padded_documents(df):\n",
    "    encoded_docs = get_encoded_ids(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, 'docs_to_embed')\n",
    "    \n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional inputs\n",
    "On top of looking at the textual data, we have a few additional columns we need to consider. Each of these we'll consider a running variable\n",
    "1. Product category\n",
    "2. Votes\n",
    "\n",
    "In the tiny sample dataset, all the records are on the same date and are in the same product category, so there's no reason to include either date or product category in the first model. We'll specify all of these as  inputs, then pass them in as a list to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_specs(df):\n",
    "    vocab_size = get_vocab_size(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, col_name = 'docs_to_embed')\n",
    "\n",
    "    n_users = len(set(df['customer_id'].values.tolist()))\n",
    "    n_products = len(set(df['product_id'].values.tolist()))\n",
    "\n",
    "    return vocab_size, max_length, n_users, n_products\n",
    "\n",
    "def get_scaled(df, col_name):\n",
    "    x = [int(x) for x in df[col_name]]\n",
    "    \n",
    "    x = np.reshape(x, (-1, 1))\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "\n",
    "    scaler_x.fit(x)\n",
    "    \n",
    "    xscale = scaler_x.transform(x)\n",
    "\n",
    "    return xscale\n",
    "\n",
    "def get_model_input_data(df):\n",
    "    \n",
    "    padded_docs = get_padded_documents(df)\n",
    "    \n",
    "    encoded_product_ids = get_encoded_ids(df, 'product_id')\n",
    "\n",
    "    votes = get_scaled(df, 'total_votes')\n",
    "    \n",
    "    return [padded_docs, votes, encoded_product_ids, df['customer_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(df):\n",
    "\n",
    "    vocab_size, max_length, n_users, n_products = get_model_input_specs(df)\n",
    "\n",
    "    ##########\n",
    "    # INPUTS #\n",
    "    ##########\n",
    "\n",
    "    doc_input = Input(shape=[max_length,], dtype='int32', name=\"Document-Input\")\n",
    "    doc_embedding = Embedding(vocab_size, output_dim = 512, name=\"Document-Embedding\", input_length=max_length)(doc_input)\n",
    "    lstm_out = LSTM(32)(doc_embedding)\n",
    "    # auxiliary output \n",
    "    aux_out = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "\n",
    "    votes_input = Input(shape=[1,], name=\"Votes-Input\") \n",
    "\n",
    "    product_input = Input(shape=[1, ], name=\"Product-Input\")\n",
    "    product_embedding = Embedding(n_products+1, 24, name=\"Product-Embedding\")(product_input)\n",
    "    \n",
    "    product_vec = Flatten(name=\"Flatten-Products\")(product_embedding)\n",
    "\n",
    "    user_input = Input(shape=[1, ], name=\"User-Input\")\n",
    "\n",
    "    ##########\n",
    "    # CONCAT #\n",
    "    ########## \n",
    "    concat = keras.layers.concatenate([lstm_out, votes_input, product_vec, user_input], \n",
    "                                      name = 'main_concat')\n",
    "\n",
    "    x1 = Dense(64, activation='relu', name='1st_post_dense')(concat)\n",
    "\n",
    "    x2 = keras.layers.Dropout(.2, name='Dropout')(x1)\n",
    "\n",
    "    x3 = Dense(32, activation='relu', name='3st_post_dense')(x2)\n",
    "\n",
    "    ###############\n",
    "    # PREDICTIONS #\n",
    "    ###############\n",
    "\n",
    "    predictions = keras.layers.Dense(1, activation='sigmoid')(x3)\n",
    "\n",
    "    #########\n",
    "    # MODEL #\n",
    "    #########\n",
    "\n",
    "    model = Model(inputs = [doc_input, votes_input, product_input, user_input], output = predictions)\n",
    "\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you defined the model, run this next cell for a few minutes. The data is too large to train in a reasonable time period for a workshop, so you're actually going to load a pre-trained model. But run this step so you can see that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 15:07:39.863368 139928220383040 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0924 15:07:39.880140 139928220383040 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0924 15:07:39.889464 139928220383040 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0924 15:07:40.756482 139928220383040 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0924 15:07:40.983199 139928220383040 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0924 15:07:41.001386 139928220383040 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0924 15:07:41.030658 139928220383040 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0924 15:07:41.045869 139928220383040 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0924 15:07:41.050820 139928220383040 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs_to_embed 12629\n",
      "product_id 1358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 15:07:42.037797 139928220383040 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0924 15:07:42.047095 139928220383040 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 333s 208ms/step - loss: 5.5387 - val_loss: 3.8660\n"
     ]
    }
   ],
   "source": [
    "model = get_model(df)\n",
    "\n",
    "data_input = get_model_input_data(df)\n",
    "\n",
    "history = model.fit(data_input, labels, epochs=1, verbose=1, validation_split=0.2)  \n",
    "\n",
    "version = 8\n",
    "\n",
    "model.save('full-model-v{}.h5'.format(version))\n",
    "\n",
    "# write to json \n",
    "json_string = model.to_json()\n",
    "with open(\"full-model-v{}.json\".format(version), \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('full-model_weights-v{}.h5'.format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
