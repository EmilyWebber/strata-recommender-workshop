{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Recommender Model\n",
    "In this notebook we'll:\n",
    "- Read in the data from disk\n",
    "- Define the preprocessing functions\n",
    "- Define the model\n",
    "- Apply the features to the model, ie fit\n",
    "- Load a pretrained model from disk\n",
    "- Run inference on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling Keras-2.1.2:\n",
      "  Successfully uninstalled Keras-2.1.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras==2.1.2\n",
      "  Using cached https://files.pythonhosted.org/packages/68/89/58ee5f56a9c26957d97217db41780ebedca3154392cb903c3f8a08a52208/Keras-2.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.16.4)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras==2.1.2) (1.3.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras -y\n",
    "!pip install keras==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "import sagemaker\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense, LSTM, Activation\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(f_name):\n",
    "    \n",
    "    df = pd.read_csv(f_name, sep='\\t', nrows = 10000)\n",
    "        \n",
    "    df['docs_to_embed'] = df['product_title'] + ', ' + df['review_headline'] + ', ' + df['review_body']\n",
    "    \n",
    "    df.drop(['product_title', 'review_headline', 'review_body'], axis=1, inplace=True)\n",
    "    \n",
    "    # remove nans\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "#     df['customer_id'] = str(df['customer_id'])\n",
    " \n",
    "    return df\n",
    "\n",
    "df = read_data('micro_data.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a model\n",
    "\n",
    "### Features to embed:\n",
    "1. Product title\n",
    "2. Review headline\n",
    "3. Review body\n",
    "4. Customer ID\n",
    "5. Product ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    labels = [1 if int(x) >= 4 else 0 for x in df['star_rating'] ]\n",
    "    return labels\n",
    "\n",
    "labels = get_label(df)\n",
    "\n",
    "def get_encoded_ids(df, id_name):\n",
    "\n",
    "    assert id_name in ['product_id', 'docs_to_embed']\n",
    "    \n",
    "    vocab_size = get_vocab_size(df, col_name = id_name)\n",
    "    \n",
    "    print (id_name, vocab_size)\n",
    "\n",
    "    docs = df[id_name].values.tolist()\n",
    "        \n",
    "    encoded_ids = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "    return np.array(encoded_ids)\n",
    "\n",
    "def get_vocab_size(df, col_name):\n",
    "    vocab_size = len(set((' ').join(df[col_name]).split()))\n",
    "    return vocab_size\n",
    "\n",
    "def get_max_length(df, col_name):\n",
    "    max_length = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        doc = row[col_name]\n",
    "        l = len(doc.split())\n",
    "        if l > max_length:\n",
    "            max_length = l\n",
    "            \n",
    "    return max_length\n",
    "\n",
    "def get_padded_documents(df):\n",
    "    encoded_docs = get_encoded_ids(df, 'docs_to_embed')\n",
    "    max_length = get_max_length(df, 'docs_to_embed')\n",
    "    \n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional inputs\n",
    "On top of looking at the textual data, we have a few additional columns we need to consider. Each of these we'll consider a running variable\n",
    "1. Product category\n",
    "2. Votes\n",
    "\n",
    "In the tiny sample dataset, all the records are on the same date and are in the same product category, so there's no reason to include either date or product category in the first model. We'll specify all of these as  inputs, then pass them in as a list to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_specs(df):\n",
    "    n_users = 9928\n",
    "    n_products = 4412\n",
    "    max_length = 2261\n",
    "    vocab_size = 34467\n",
    "\n",
    "    return vocab_size, max_length, n_users, n_products\n",
    "\n",
    "\n",
    "# def get_model_input_specs(df):\n",
    "#     vocab_size = get_vocab_size(df, 'docs_to_embed')\n",
    "#     max_length = get_max_length(df, col_name = 'docs_to_embed')\n",
    "\n",
    "#     n_users = len(set(df['customer_id'].values.tolist()))\n",
    "#     n_products = len(set(df['product_id'].values.tolist()))\n",
    "\n",
    "#     return vocab_size, max_length, n_users, n_products\n",
    "\n",
    "def get_scaled(df, col_name):\n",
    "    x = [int(x) for x in df[col_name]]\n",
    "    \n",
    "    x = np.reshape(x, (-1, 1))\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "\n",
    "    scaler_x.fit(x)\n",
    "    \n",
    "    xscale = scaler_x.transform(x)\n",
    "\n",
    "    return xscale\n",
    "\n",
    "def get_model_input_data(df):\n",
    "    \n",
    "    padded_docs = get_padded_documents(df)\n",
    "    \n",
    "    encoded_product_ids = get_encoded_ids(df, 'product_id')\n",
    "\n",
    "    votes = get_scaled(df, 'total_votes')\n",
    "    \n",
    "    return [padded_docs, votes, encoded_product_ids, df['customer_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(df):\n",
    "\n",
    "    vocab_size, max_length, n_users, n_products = get_model_input_specs(df)\n",
    "\n",
    "    ##########\n",
    "    # INPUTS #\n",
    "    ##########\n",
    "\n",
    "    doc_input = Input(shape=[max_length,], dtype='int32', name=\"Document-Input\")\n",
    "    doc_embedding = Embedding(vocab_size, output_dim = 512, name=\"Document-Embedding\", input_length=max_length)(doc_input)\n",
    "    lstm_out = LSTM(32)(doc_embedding)\n",
    "    # auxiliary output \n",
    "    aux_out = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "\n",
    "    votes_input = Input(shape=[1,], name=\"Votes-Input\") \n",
    "\n",
    "    product_input = Input(shape=[1, ], name=\"Product-Input\")\n",
    "    product_embedding = Embedding(n_products+1, 24, name=\"Product-Embedding\")(product_input)\n",
    "    \n",
    "    product_vec = Flatten(name=\"Flatten-Products\")(product_embedding)\n",
    "\n",
    "    user_input = Input(shape=[1, ], name=\"User-Input\")\n",
    "\n",
    "    ##########\n",
    "    # CONCAT #\n",
    "    ########## \n",
    "    concat = keras.layers.concatenate([lstm_out, votes_input, product_vec, user_input], \n",
    "                                      name = 'main_concat')\n",
    "\n",
    "    x1 = Dense(64, activation='relu', name='1st_post_dense')(concat)\n",
    "\n",
    "    x2 = keras.layers.Dropout(.2, name='Dropout')(x1)\n",
    "\n",
    "    x3 = Dense(32, activation='relu', name='3st_post_dense')(x2)\n",
    "\n",
    "    ###############\n",
    "    # PREDICTIONS #\n",
    "    ###############\n",
    "\n",
    "    predictions = keras.layers.Dense(1, activation='sigmoid')(x3)\n",
    "\n",
    "    #########\n",
    "    # MODEL #\n",
    "    #########\n",
    "\n",
    "    model = Model(inputs = [doc_input, votes_input, product_input, user_input], output = predictions)\n",
    "\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you defined the model, run this next cell for a few minutes. The data is too large to train in a reasonable time period for a workshop, so you're actually going to load a pre-trained model. But run this step so you can see that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs_to_embed 34467\n",
      "product_id 4412\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "  96/8000 [..............................] - ETA: 28:37 - loss: 11.4170"
     ]
    }
   ],
   "source": [
    "model = get_model(df)\n",
    "\n",
    "data_input = get_model_input_data(df)\n",
    "\n",
    "history = model.fit(data_input, labels, epochs=1, verbose=1, validation_split=0.2)  \n",
    "\n",
    "version = 5\n",
    "\n",
    "model.save('/home/ec2-user/SageMaker/full-model-v{}.h5'.format(version))\n",
    "\n",
    "# write to json \n",
    "json_string = model.to_json()\n",
    "with open(\"full-model-v{}.json\".format(version), \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('full-model_weights-v{}.h5'.format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('''gzip -c full-model-{}3.h5 > model.h5.gz'''.format(version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained model from disk & run predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 11:26:35.286799 139976863463232 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0924 11:26:35.304689 139976863463232 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0924 11:26:35.307956 139976863463232 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0924 11:26:35.495209 139976863463232 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0924 11:26:35.501175 139976863463232 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0924 11:26:35.584716 139976863463232 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0924 11:26:36.013079 139976863463232 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0924 11:26:36.020070 139976863463232 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('full-model-v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs_to_embed 7912\n",
      "product_id 787\n"
     ]
    }
   ],
   "source": [
    "# here we'll run predictions on a sample from the training set\n",
    "data_input = get_model_input_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected Document-Input to have shape (2261,) but got array with shape (394,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fe60bfc2d3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected Document-Input to have shape (2261,) but got array with shape (394,)"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
